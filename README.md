# Sign-Language-Recognition
In this sign language Recognition project we have created a detector,
which detects alphabets from A to Z and numbers from 1 to 10.
We have used convolutional neural networks (CNNs) to build and train a
model which can classify Sign Languages from the images of hands. Our model has achieved a train accuracy of 96.32%.
We have used CV to detect palm images from our webcam and then feed that image into our model to detect the sign. 

* You can find the Jupyter Notebook **[here](https://colab.research.google.com/drive/1rE4Cr-GYEhs8qSjL-BkFGa31YibtnWfP)**. 

* Dataset was avilable online. You can find it **[here](https://www.massey.ac.nz/~albarcza/gesture_dataset2012.html)**.

Sign Detector File contains the code of capturing image from webcam and predicting the output. Sign Language Regognition file contains the code for our model.
## Team Members :
1.  <a href ="https://github.com/Sudhanshu2920">Sudhanshu Pandey</a> 
2.  <a href="https://github.com/GvarunG">Varun Gupta</a>
3.  <a href ="https://github.com/singhyashveer/">Yashveer Singh</a>

